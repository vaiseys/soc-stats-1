---
title: "Soc 722"
author: "Stephen Vaisey"
subtitle: "Spring 2025"
execute: 
  echo: true
  warning: false
  freeze: auto
format: 
  revealjs:
    theme: [default, inverse.scss, custom.scss]
    slide-number: true
    embed-resources: true
editor: source
revealjs-plugins:
  - revealjs-text-resizer
editor_options: 
  chunk_output_type: console
---

# Introduction {.inverse}

## Objective

This is the first in a two-course sequence designed to help you become competent quantitative researchers in sociology.

This includes learning proper **decision making**, **explanation**, **computation**, **visualization**, and **interpretation**.

## Schedule

We will normally meet Tuesdays. Please note that we will be meeting on the following **Thursdays** (not Tuesdays) because of my travel schedule:

- today
- January 30
- February 6
- February 13
- March 20

## Final exam

The exam will be **remote** on April 30 from 9am-12pm.

## Syllabus

The syllabus is [here](https://docs.google.com/document/d/1SnKMREAejFdr-MpejuxieZ3nPz7ftdHDoWiRv2aRtQc/edit?usp=sharing). Please be sure to read it.

##

:::{.r-fit-text .absolute top="20%"}
Questions?
:::

# Data and Variables

## Data structure

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(gapminder)
library(tinytable)
library(knitr)
library(kableExtra)

theme_set(theme_light())

options(pillar.width = 70)  # having some trouble with glimpse overflow
```

```{r}
#| echo: false
data(gapminder)
head(gapminder, n = 10) |> 
  kbl("html") |>
  kable_styling(font_size = 24)
```

Tidy format: columns contain **variables**, each row is an **observation**.

## Untidy data

```{r}
#| echo: false

untidy <- gapminder |> 
  pivot_wider(values_from = c(lifeExp, pop, gdpPercap),
              names_from = year)

head(untidy, n = 12) |> 
  kbl("html") |>
  kable_styling(font_size = 24)
```


## Types of variables

|              |                                    |
|--------------|------------------------------------|
| **Ratio**    | dollars; points (e.g., basketball) |
| **Interval** | degrees Celsius                    |
| **Ordinal**  | clothing sizes; Likert scales      |
| **Nominal**  | race; sex; country                 |

: {.striped}

The first two types are **continuous** or **numeric**. The second two types are **categorical**. Ordinal variables are often treated as numeric and this is usually fine.

## 

Let's investigate this using the `gapminder` data. First of all, we'll keep only the most recent (2007) data.

```{r}
d <- gapminder |>               
  filter(year == max(year)) |> # keep 2007
  select(-year)                # don't need column
```

## 

```{r}
#| echo: false

head(d, n = 10) |> 
  kbl("html") |>
  kable_styling(font_size = 24)
```

What kinds of variables are these?

## The origins of "statistics"

The word _statistics_ comes from the fact that it was information about _the state_. We'll focus on information like this for now rather than thinking about samples of individuals.

## Visualization basics

Consider two types of plots

- univariate plots

- bivariate plots

These are also types of **distributions.**

## Univariate plots

## Density plot

```{r}
#| output-location: slide

ggplot(d,
       aes(x = gdpPercap)) +
  geom_density()
```

## Histogram (1)

```{r}
#| output-location: slide

ggplot(d,
       aes(x = gdpPercap)) +
  geom_histogram(binwidth = 5000,
                 boundary = 0,
                 color = "white")
```

## Histogram (2)

```{r}
#| output-location: slide

ggplot(d,
       aes(x = lifeExp)) +
  geom_histogram(binwidth = 5,
                 boundary = 0,
                 color = "white")
```

## Bar graph (univariate)

```{r}
#| output-location: slide

ggplot(d,
       aes(x = continent)) +
  geom_bar()
```

## Bivariate plots

## Scatter plot

```{r}
#| output-location: slide

ggplot(d,
       aes(x = gdpPercap,
           y = lifeExp)) +
         geom_point()
```

## Bar graph (bivariate)

```{r}
#| output-location: slide

d |> 
  group_by(continent) |> 
  summarize(GDP = mean(gdpPercap)) |>
  ggplot(aes(x = continent,
             y = GDP)) +
  geom_bar(stat = "identity")
```

## Strip plot

```{r}
#| output-location: slide

ggplot(d,
       aes(x = gdpPercap,
           y = continent)) +
  geom_point(alpha = .3)
```

## Jittered strip plot

```{r}
#| output-location: slide

ggplot(d,
       aes(x = gdpPercap,
           y = continent)) +
  geom_jitter(height = .1,
              width = .1,
              alpha = .2)
```

## Time plots (bivariate)

Let's go back to the full data and look at trends in Oceania.

```{r}
#| output-location: slide

gapminder |> 
  filter(continent == "Oceania") |> 
  ggplot(aes(x = year,
             y = lifeExp,
             group = country,
             color = country)) +
  geom_line()
```

## {#slide-yt1-id data-menu-title="Your turn"}

:::{.r-fit-text .absolute top="20%"}
Your turn!
:::

## Homework

1. Set up a public GitHub repository for this class
2. Share it with the class on Slack
3. Create a Quarto document (`HW1.qmd`) making several visualizations using data of your choice
4. Render it to html (`HW1.html`)
5. Push it to your repository by midnight before the next meeting

## Quarto tips

[for later]

# Surveys, samples, and probability {.inverse}

## Populations and samples

Studying **populations** is nice:

- all countries in the world
- all states in the US
- all cities in a state

However, we cannot study (say) all adults in a country. So we usually work with **samples**. This raises the issue of using _samples_ to make inferences about _populations_.

## Simple random sampling

Sampling where every eligible case has an equal probability of selection. 

::::{.columns}

:::{.column width="50%}

![](images/sampling-balls.jpg)
:::

:::{.column width="50%}
:::{.callout-note}
In real-life surveys, SRS is pretty uncommon. But it's important as a baseline!
:::
:::

::::

## Simulations

We can use **simulations** to build intuition about sampling.

A _simulation_ is when we make up "true data", hide it from ourselves, and see how well we can figure out the the truth using some procedure.

## Simple survey

Imagine a city of 100,000 adults. Of these, 70,000 (i.e., 70%) have at least one child.

How close could we get to this number by drawing different random samples?

## A first simulation

Let's set up the "true" population:

```{r}
population <- tibble(id = 1:1e5) |> # initialize with 100K rows
  mutate(parent = if_else(id <= 70000, "Yes", "No"))  # first 70K are "yes"
```

This simple code makes the first 70,000 rows "yes" and the next 30,000 "no." We now know the "truth", which we can use for comparison.

## Visualizing the population

```{r}
#| echo: false

ggplot(population,
       aes(x = parent)) +
  geom_bar()
```

## Data types

Because of the way we created it, `parent` will be a character `<chr>` variable. We often use `1` to mean "yes" and `0` to mean "no" in statistics. We could add a numeric version of `parent` as follows.

```{r}
population <- population |> 
  mutate(parent_num = if_else(parent == "Yes", 1L, 0L))
```

:::{.callout-tip}
Assigning an object to its "old" name allows you to add things to the original object. In this case, we are adding a new column, `parent_num`.
:::

## Checking data type 

We can use `glimpse()` to easily see what type of variable things are.

```{r}
glimpse(population)
```

::: {.callout-tip}
`glimpse()` is very useful. It shows you your data "sideways" so you can see information about all the columns (which are shown as rows).
:::

## Implications of data type

Data type affects what we can do to a variable (or column). For example, we can take the mean (average) of a set of _numbers_ but we can't take the mean of a set of _characters_.

::::{.columns}

:::{.column width="55%"}
```{r}
#| error: true

population |>
  summarize(mean1 = mean(parent),
            mean2 = mean(parent_num))
```
:::

:::{.column width="45%"}
:::{.callout-note}
In R, `NA` stands for "not available" and means that the data are _missing_. This cell is missing because what we asked for couldn't be calculated.
:::
:::

::::

## Aside: doing stuff to a variable {.incremental}

We can pick out a column to operate on in two ways:

### The tidy way

```{r}
population |> pull(parent_num) |> mean()
```

### The base R way

```{r}
mean(population$parent_num)
```

Now back to our story...

## Parameters and statistics

In our city, 70% is the **population parameter** because exactly 70,000 out of 100,000 people actually have at least one child.

We can't afford to ask everyone, though. So what if we asked, say, 1000 randomly selected adults. Then we could compute the proportion of the _sample_ that has a child. This would be a **sample statistic**. We use _sample statistics_ to **make inferences** about _population parameters_.

## Drawing a sample

```{r}
set.seed(722)
my_sample <- population |> 
  slice_sample(n = 1000,
               replace = FALSE)
```

Sampling is a _random_ process. We will get a different result every time. By using `set.seed()`, we ensure we get the same "random" result every time the code is run.

:::{.callout-note}
Sampling theory is based on sampling _with replacement_. However, to make it more straightforward, we will use sampling _without replacement_ here.
:::

## The sample statistic

To estimate the _population proportion_, we will use the _sample proportion_.

```{r}
my_sample |> 
  group_by(parent) |> 
  summarize(n = n())
```

In our sample, 682 people are parents. This is 68.2%, which isn't _exactly_ the 70% in the population. This is because we randomly sampled from our population. It could be higher or lower.

## Repeating the experiment

In real life, we only get to sample _once_. Sampling is expensive! But since this is just a simulation, we can ask what would happen if we sampled 1000 people many, many times.

## A custom function

We can first make a **function** that does what we want _once_. This is hard at first but usually pays off.

```{r}
get_count <- function(n = 1000) {       # default n = 1000
  slice_sample(population, n = n) |>    # take a sample
    summarize(sum = sum(parent_num)) |> # count the parents
    as.integer()                        # save the number
}
```

:::{.callout-tip}
If you run all the code up to here, you can call `get_count()` interactively in the console many times to get a feel for it.
:::

## Iterating

This would seem to make sense, but it doesn't work.

```{r}
set.seed(722)
my_bad_samples <- tibble(
  sample_id = 1:100,
  samp_count = get_count(n = 1000))
head(my_bad_samples)
```

## Iterating with `rowwise`

```{r}
set.seed(722)
my_samples <- tibble(
  sample_id = 1:100) |> 
  rowwise() |> 
  mutate(samp_count = get_count(n = 1000))
head(my_samples, n = 3)
```

:::{.callout-tip}
I don't _need_ `n = 1000` because I set it as the default when I made my function.
:::

## Plotting the results

```{r}
#| output-location: slide
#| fig-height: 5

ggplot(my_samples,
       aes(x = samp_count)) +
  geom_histogram(aes(y = ..density..), # for overlay
                 boundary = 697.5,     # why would I choose this?
                 binwidth = 5,         # somewhat arbitrary
                 color = "white",
                 fill = "gray") +
  geom_density(color = "#36454f",      # overlay density
               linewidth = 1,
               alpha = .5)      
```

## Repeating with 2,500 samples

```{r}
#| echo: false
#| cache: true
#| fig-align: center
#| fig-height: 5

set.seed(722)
my_many_samples <- tibble(
  sample_id = 1:2500) |> 
  rowwise() |> 
  mutate(samp_count = get_count())

ggplot(my_many_samples,
            aes(x = samp_count)) +
  geom_histogram(aes(y = ..density..),
                 boundary = 697.5,  # why would I choose this?
                 binwidth = 5,      # somewhat arbitrary
                 color = "white",
                 fill = "gray") +
  scale_x_continuous(breaks = seq(600, 800, 10)) +
  geom_density(color = "#36454f",    # overlay density
               linewidth = 1,
               alpha = .5)    
```

## Sample size and number of samples

When we are doing simulations like this, it can be easy to confuse the **sample size** (1000) with the **number of samples** in our simulation (2500). They are not the same thing!

The _sample size_ is the number of people we would survey "in the real world".

The _number of samples_ is how many times we want to run our simulated experiment.

## How accurate are we?

We will do this formally later. But now we can quantify how accurately a _sample proportion_ of 1000 people might estimate this _population proportion_ by using the **interquartile range**. This is how wide the middle half of the data is.

```{r}
my_many_samples |> pull(samp_count) |> quantile(c(.25, .75))
my_many_samples |> pull(samp_count) |> IQR()
```

:::{.callout-tip}
## Reminder
Remember: in real life we only get _one_ of these samples. 
:::

## Visualizing IQR

```{r}
#| echo: false

ggplot(my_many_samples,
            aes(x = samp_count)) +
  geom_density(fill = "gray",
               color = NA) +
  scale_x_continuous(breaks = seq(600, 800, 10)) + 
  geom_vline(xintercept = quantile(my_many_samples$samp_count,
                                   c(.25, .75)),
             color = "#CC0000")

```

## Sample size

Remember that we drew a sample of 1000 people to estimate our _sample proportions_. What if we had different sample sizes? Let's compare the following:

- N = 60
- N = 250
- N = 1000

## Visualizing "accuracy"

```{r}
#| echo: false
#| fig-align: center

set.seed(722)
my_n60_samples <- tibble(
  sample_id = 1:2500,
  sample_size = "N = 60") |> 
  rowwise() |> 
  mutate(samp_count = get_count(n = 60),
         samp_prop = samp_count / 60)     # proportion

my_n250_samples <- tibble(
  sample_id = 1:2500,
  sample_size = "N = 250") |> 
  rowwise() |> 
  mutate(samp_count = get_count(n = 250),
         samp_prop = samp_count / 250)

my_many_samples <- my_many_samples |> # adding the group var and prop
  mutate(sample_size = "N = 1000",
         samp_prop = samp_count / 1000)

samp_size_compare <- 
  bind_rows(my_n60_samples,
            my_n250_samples,
            my_many_samples)

ggplot(samp_size_compare,
       aes(x = samp_prop,
           group = sample_size,
           color = sample_size)) +
  geom_density()
```

:::{.callout-tip}
## Proportions
The x-axis is now _proportion_ because we can no longer compare raw counts.
:::

## Comparing IQR

The interquartile ranges (i.e., widths of the middle half of the data) decrease a lot with sample size.

::::{.columns}

:::{.column width="35%"}
```{r}
#| echo: false

samp_size_compare |>              
  group_by(sample_size) |>        
  summarize(IQR = IQR(samp_prop))
```
:::

:::{.column width="65%"}
:::{.callout-warning}
We will explore these issues more formally very soon using the concepts **sampling distribution** and **standard error**. For now, the goal is to understand how to use simulations to build qualitative intuition about sample size.
:::
:::

::::

## Thinking with real data: GSS

The General Social Survey is a **repeated cross-sectional** survey that has been fielded every year or other year since 1972. It is the "Hubble Telescope" of sociology!

## Accessing the GSS in R

Install Kieran Healy's `gssr` package using the code below. You only have to do this once.

```{r}
#| eval: false

# Install 'gssr' from 'ropensci' universe
install.packages('gssr', repos =
  c('https://kjhealy.r-universe.dev', 'https://cloud.r-project.org'))

# Also recommended: install 'gssrdoc' as well
install.packages('gssrdoc', repos =
  c('https://kjhealy.r-universe.dev', 'https://cloud.r-project.org'))
```

## Getting the 2022 survey

```{r}
library(gssr)

gss2022 <- gss_get_yr(year = 2022) |> # get 2022
  haven::zap_labels()                 # remove Stata value labels

glimpse(gss2022)
```

## Introducing `abany`

The GSS `abany` item asks "Please tell me whether or not you think it should be possible for a pregnant woman to obtain a legal abortion if the woman wants it for any reason?" The answers are "yes" (`1`) and "no" (`2`).

There is also a web version of this in 2022: `abanyng`. We will drop this for now.

:::{.callout-warning}
We do not want variables coded `1` and `2`. As we will see later, it's better if (almost) all variables have a meaningful `0` value.
:::

## Wrangling `abany`

::::{.columns}

:::{.column width="43%"}

```{r}
d <- gss2022 |> 
  select(abany) 

d |> group_by(abany) |> 
  summarize(n = n())
```
:::

:::{.column width="57%"}
```{r}
d <- d |> 
  drop_na() |>   # drop NA values
  mutate(abany = case_match(abany,
                            1 ~ 1,
                            2 ~ 0 ))

d |> pull(abany) |> table()
```
:::

::::

:::{.callout-tip}
`case_match()` is useful for recoding variables.
:::

## `abany` sample proportion

We can use `mean()` to calculate the _sample proportion_.

```{r}
mean(d$abany)
```

We find that `r round(mean(d$abany), 3)*100`% of our sample supports abortion rights for any reason.

## Inference

How close is this _sample statistic_ to the _population parameter_? We'll never know. 

We can use a simulation to give us a sense of what kind of accuracy is possible with a sample of `r nrow(d)` respondents.

## Random number functions

We could build an imaginary US adult population where 59.4% of adults support abortion rights. But we can instead use **random number functions** to draw a sample from an _infinite_ population instead.

```{r}
set.seed(722)
rbinom(1345, 1, .594) |> # random sample of 1345
  mean()                 # take the mean
```

:::{.callout-tip}
## Why 59.4%?
Why assume that the population parameter is the same as the sample statistic? Because we are really interested in how _widely spread_ the simulations are and there's no more reasonable value to choose.
:::

## Taking many samples (again)

Let's do this 5000 times and collect the results.

```{r}
set.seed(722)
gss_sims <- tibble(
  sim_id = 1:5000) |> 
  rowwise() |> 
  mutate(samp_prop = mean(rbinom(1345, 1, .594)))
```

The IQR tells us how spread out the middle half of the estimates are.

```{r}
gss_sims |> pull(samp_prop) |> IQR()
```

Thus, with 1345 cases, half of the sample proportions will be within approximately 1.9 points of the true value.

## Visualize

```{r}
#| echo: false

ggplot(gss_sims,
       aes(x = samp_prop)) +
  geom_density(fill = "gray",
               color = NA) +
  scale_x_continuous(breaks = seq(.50, .70, .01)) + 
  geom_vline(xintercept = quantile(gss_sims$samp_prop,
                                   c(.025, .25, .75, .975)),
             color = "#CC0000") +
  labs(caption = "lines at 2.5th, 25th, 75th, and 97.5th percentiles")
```

## Summary

We still don't know the true value, of course. We are _probably_ within a couple of points of the true value. But we could be 3 (or _possibly_ more) points away.

![](images/never-certain.jpg){fig-align="center"}

## Recap

- we want to know about **populations**
- we end up having to use **samples** 
- samples are _random_ subsets of the population that are expensive to collect
- the larger the sample, the more accurately we can **infer** the population proportion
- we can use **simulations** to understand how this works.


## Homework

- make a fake population of at least 100,000 inhabitants _OR_ you can use random number functions
- make some people do/think/believe/are X (`1`) and some people not-X (`0`)
- write a function that samples from that population using at least three different sample sizes 
- plot and interpret your results
- push it to GitHub the night before the next lecture

Message me on Slack if you are struggling!
